{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "royal-separate",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e1912a-b6d5-48cf-898c-0f8d1e6ee000",
   "metadata": {},
   "source": [
    "## Main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "limited-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-violence",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fewer-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"data\"\n",
    "dst = \"data\"\n",
    "fname = \"mapOSR_data_V5_9_3_220419_coded\"\n",
    "data = pd.read_csv(join(src, fname + \"_raw.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-lancaster",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "infrared-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns & drop unneccesary columns\n",
    "data = data\\\n",
    "    .rename(columns={'V=Action':'Action',\n",
    "                     'V=Method':'Method',\n",
    "                     'V=Discipline':'Discipline',\n",
    "                     'V=Group':'Group',\n",
    "                     'V=Geo-Scope':'Geo'})\\\n",
    "    .drop(columns=['Unnamed: 25', \"Coder\", \"IS_Kommentar\",\n",
    "                   \"DuplicateTitle\", \"DuplicateDOI\",\n",
    "                   \"Comments\", \"Book Author Full Names\", \"Group Authors\",\n",
    "                   \"Publication Type\", \"Cited References\", \"Book DOI\",\n",
    "                   \"Book Editors\"])\n",
    "\n",
    "data[\"Abstract\"] = data[\"Abstract\"].apply(lambda x: x.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "accessory-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of wrong codings and their respective corrections\n",
    "miscodings_action = {\n",
    "        'policies':'openpolicies', # actions\n",
    "        'openacces':'openaccess', # actions\n",
    "        'missing':np.nan # general missing entry code\n",
    "}\n",
    "\n",
    "miscodings_method = {\n",
    "        'interviews':'interview', # methods\n",
    "        'other(casestudies)':'othermeth', # methods\n",
    "        'desk-review':'othermeth', # methods\n",
    "        'deskresearch':'othermeth', # methods\n",
    "        'bibliometric':'biblio', # methods\n",
    "        'documentanalysis':'othermeth', # methods\n",
    "        'other':'othermeth',\n",
    "        'missing':np.nan # general missing entry code\n",
    "}\n",
    "\n",
    "miscodings_discipline = {\n",
    "        'socsci':'socscie', # disciplines\n",
    "        'socscie(LIS)':'socscie', # disciplines\n",
    "        'socscie(psychology)':'socscie', # disciplines\n",
    "        'socscie?(informationsciences)':'socscie', # disciplines\n",
    "        'soscie':'socscie', # disciplines\n",
    "        'socsie':'socscie', # disciplines\n",
    "        'nonspecific':'nonspecificdisc',\n",
    "        'missing':np.nan # general missing entry code\n",
    "}\n",
    "\n",
    "miscodings_group = {\n",
    "        'librarians':'librarian', # group\n",
    "        'publishers':'publisher', # group\n",
    "        'reseaercher':'researcher', # group\n",
    "        'researchers':'researcher', # group\n",
    "        'all':'othergroup', # group\n",
    "        'none':'othergroup', # group\n",
    "        'other':'othergroup',\n",
    "        'missing':np.nan # general missing entry code\n",
    "}\n",
    "\n",
    "miscodings_geo = {\n",
    "        'TW':'TWN', # geo\n",
    "        'IR':'IRN', # geo\n",
    "        'IN':'IND', # geo\n",
    "        'Italy':'ITA', # geo\n",
    "        'PK':'PAK', # geo\n",
    "        'SI':'SVN', # geo\n",
    "        'LA':'SA', # geo\n",
    "        'CHI':'CHN',\n",
    "        'CND':'CAN',\n",
    "        'DEN':'DNK',\n",
    "        'NDL':'NLD',\n",
    "        'SUI':'CHE',\n",
    "        'HNK':'HKG',\n",
    "        'GNR':'GRL',\n",
    "        'nonspecific':'nonspecificgeo',\n",
    "        'missing':np.nan # general missing entry code\n",
    "}\n",
    "\n",
    "miscodings = {\n",
    "    \"Action\":miscodings_action,\n",
    "    \"Method\":miscodings_method,\n",
    "    \"Discipline\":miscodings_discipline,\n",
    "    \"Group\":miscodings_group,\n",
    "    \"Geo\":miscodings_geo\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "integrated-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(list_string, cat_col):\n",
    "    '''\n",
    "    Takes a string containing a list of encodings separated by semicolons,\n",
    "    cleans the list, splits it into different entries and returns a list\n",
    "    of entries. Also corrects misspellings along the way.\n",
    "    '''\n",
    "    \n",
    "    if list_string != list_string: # NaN check\n",
    "        return np.nan\n",
    "    \n",
    "    list_string = list_string.strip(';') # remove trailing \";\"\n",
    "    list_string = list_string.replace(':', ';') # replace colon with semicolon\n",
    "    list_string = list_string.replace(',', ';') # replace comma with semicolon\n",
    "    raw_entries = list_string.split(';') # split list along \";\"\n",
    "    \n",
    "    entries = []\n",
    "    for e in raw_entries:\n",
    "        category = \"\"\n",
    "        e = e.replace(' ', '') # remove white spaces\n",
    "        if '=' in e: # remove leading column code letter\n",
    "            try:\n",
    "                category, e = e.split('=')\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if e in miscodings[cat_col].keys(): # clean up wrong encodings\n",
    "            e = miscodings[cat_col][e]\n",
    "        if e != '' and e == e:\n",
    "            entries.append(e)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "atomic-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the five coded columns\n",
    "cols = ['Action', 'Method', 'Discipline', 'Group', 'Geo']\n",
    "for col in cols:\n",
    "    data[col] = data[col].apply(split_list, args=[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-airfare",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-retailer",
   "metadata": {},
   "source": [
    "Look at the remaining categories in each coded column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "prescription-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_actions = {\n",
    " 'openaccess',\n",
    " 'opendata',\n",
    " 'openeducation',\n",
    " 'openevaluation',\n",
    " 'openmethod',\n",
    " 'openparticipation',\n",
    " 'openpolicies',\n",
    " 'openscience',\n",
    " 'opensoftware',\n",
    " 'opentools'\n",
    "}\n",
    "actions = []\n",
    "for a in data['Action']:\n",
    "    if a == a:\n",
    "        actions += a\n",
    "actions = set(actions)\n",
    "\n",
    "assert actions == expected_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "soviet-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_methods = {\n",
    "    'biblio',\n",
    "    'documentreview',\n",
    "    'interview',\n",
    "    'othermeth',\n",
    "    'survey'\n",
    "}\n",
    "\n",
    "methods = []\n",
    "for m in data['Method']:\n",
    "    if m == m:\n",
    "        methods += m\n",
    "methods = set(methods)\n",
    "\n",
    "assert methods == expected_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "comparative-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_disciplines = {\n",
    "    'natscie',\n",
    "    'engtech',\n",
    "    'med',\n",
    "    'agric',\n",
    "    'socscie',\n",
    "    'hum',\n",
    "    'nonspecificdisc'\n",
    "}\n",
    "\n",
    "disciplines = []\n",
    "for d in data['Discipline']:\n",
    "    if d == d:\n",
    "        disciplines += d\n",
    "disciplines = set(disciplines)\n",
    "\n",
    "assert disciplines == expected_disciplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "patient-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_groups = {\n",
    " 'researcher',\n",
    " 'librarian',\n",
    " 'university',\n",
    " 'unisupportstaff',\n",
    " 'publisher',\n",
    " 'policy',\n",
    " 'funder',\n",
    " 'business',\n",
    " 'practitioner',\n",
    " 'othergroup'\n",
    "}\n",
    "\n",
    "groups = []\n",
    "for g in data['Group']:\n",
    "    if g == g:\n",
    "        groups += g\n",
    "groups = set(groups)\n",
    "\n",
    "assert groups == expected_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "conventional-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_geo = set(list(pd.read_csv(join(src, \"country_codes.csv\"), \n",
    "                        usecols=[\"iso_a3\"],\n",
    "                        na_values=[\"none\"])[\"iso_a3\"].fillna(\"NA\").values)) \\\n",
    "    | {\"nonspecificgeo\"}\n",
    "\n",
    "geo = []\n",
    "for g in data['Geo']:\n",
    "    if g == g:\n",
    "        geo += g\n",
    "geo = set(geo)\n",
    "\n",
    "assert len(geo.difference(expected_geo)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-japan",
   "metadata": {},
   "source": [
    "### Dummy code the coded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "extraordinary-south",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48364/2318456402.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['{}_{}'.format(colname, entry)] = data[colname].apply(lambda x: entry in x if x == x else False)\n"
     ]
    }
   ],
   "source": [
    "for entries, colname in zip([actions, methods, disciplines, groups, geo], cols):\n",
    "    for entry in entries:\n",
    "        if entry == entry:\n",
    "            data['{}_{}'.format(colname, entry)] = data[colname].apply(lambda x: entry in x if x == x else False)\n",
    "            data['{}_{}'.format(colname, entry)] = data['{}_{}'.format(colname, entry)].replace({True:1, False:0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-vermont",
   "metadata": {},
   "source": [
    "### Export the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "supposed-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(join(dst, fname + \"_clean.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb82e1-26fd-4792-bec0-5c2417b3f7e5",
   "metadata": {},
   "source": [
    "## Interrater reliability dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e67caf9-c5df-41b6-8d59-6895f187d643",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09277abf-1b01-45ca-b337-a16b17a99e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"data\"\n",
    "dst = \"data\"\n",
    "fname = \"mapOSR_interrater_reliability\"\n",
    "data = pd.read_csv(join(src, fname + \"_raw\" + \".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e3a79-5c7c-4e63-9e81-81af246095b4",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "757382f6-f8b9-40a3-91c9-f33531b327cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(list_string, cat_col):\n",
    "    '''\n",
    "    Takes a string containing a list of encodings separated by semicolons,\n",
    "    cleans the list, splits it into different entries and returns a list\n",
    "    of entries. Also corrects misspellings along the way.\n",
    "    '''\n",
    "    \n",
    "    if list_string != list_string: # NaN check\n",
    "        return np.nan\n",
    "    \n",
    "    list_string = list_string.strip(';') # remove trailing \";\"\n",
    "    list_string = list_string.replace(':', ';') # replace colon with semicolon\n",
    "    list_string = list_string.replace(',', ';') # replace comma with semicolon\n",
    "    raw_entries = list_string.split(';') # split list along \";\"\n",
    "    \n",
    "    entries = []\n",
    "    for e in raw_entries:\n",
    "        category = \"\"\n",
    "        e = e.replace(' ', '') # remove white spaces\n",
    "        if '=' in e: # remove leading column code letter\n",
    "            try:\n",
    "                category, e = e.split('=')\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if e in miscodings[cat_col].keys(): # clean up wrong encodings\n",
    "            e = miscodings[cat_col][e]\n",
    "        if e != '' and e == e:\n",
    "            entries.append(e)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be5195f6-a1f8-4eeb-9745-a80ca75dfd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of wrong codings and their respective corrections\n",
    "miscodings_action = {\n",
    "        'openpolicy':'openpolicies', # actions\n",
    "        'openacces':'openaccess', # actions\n",
    "        'opentool':'opentools', # actions\n",
    "        'missing':np.nan # general missing entry code\n",
    "}\n",
    "miscodings_method = {\n",
    "        'documentanalysis':'documentreview', # methods\n",
    "        'bibliometric':'biblio', # methods\n",
    "        'other':'othermeth',\n",
    "        'none':np.nan,\n",
    "        'missing':np.nan # general missing entry code\n",
    "}\n",
    "miscodings_discipline = {\n",
    "        'soscie':'socscie', # disciplines\n",
    "        'socscie(psychology)':'socscie', # disciplines\n",
    "        'all':'nonspecificdisc', # discipline\n",
    "        'nonspecific':'nonspecificdisc',\n",
    "        'none':np.nan,\n",
    "        'missing':np.nan # general missing entry code\n",
    "}\n",
    "miscodings_group = {\n",
    "        'librarians':'librarian', # group\n",
    "        'publishers':'publisher', # group\n",
    "        'researchers':'researcher', # group\n",
    "        'other':'othergroup',\n",
    "        'none':np.nan,\n",
    "        'missing':np.nan # general missing entry code\n",
    "}\n",
    "miscodings_geo = {\n",
    "        'IN':'IND', # geo\n",
    "        '?':np.nan, # geo\n",
    "        'LA':'SA', # geo\n",
    "        'nonspecific':'nonspecificgeo',\n",
    "        'all':'nonspecificgeo',\n",
    "        'none':np.nan,\n",
    "        'missing':np.nan # general missing entry code\n",
    "}\n",
    "\n",
    "miscodings = {\n",
    "    'c1_action':miscodings_action,\n",
    "    'c2_action':miscodings_action,\n",
    "    'c1_method':miscodings_method,\n",
    "    'c2_method':miscodings_method,\n",
    "    'c1_discipline':miscodings_discipline,\n",
    "    'c2_discipline':miscodings_discipline,\n",
    "    'c1_group':miscodings_group,\n",
    "    'c2_group':miscodings_group,\n",
    "    'c1_geo':miscodings_geo,\n",
    "    'c2_geo':miscodings_geo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fe1c48c-f735-486b-863c-75e6f2ce275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the five coded columns\n",
    "cols = ['c1_action', 'c1_method', 'c1_discipline', 'c1_group', 'c1_geo',\n",
    "        'c2_action', 'c2_method', 'c2_discipline', 'c2_group', 'c2_geo']\n",
    "for col in cols:\n",
    "    data[col] = data[col].apply(split_list, args=[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b35818-1911-44ad-98a1-b1bb87d4b2c0",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9492840b-ed17-4da5-b6b4-1d81a4608241",
   "metadata": {},
   "source": [
    "Look at the remaining categories in each coded column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f463af8-8b83-41da-8c4d-d2735ab6e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_actions = {\n",
    " 'openaccess',\n",
    " 'opendata',\n",
    " 'openeducation',\n",
    " 'openevaluation',\n",
    " 'openmethod',\n",
    " 'openparticipation',\n",
    " 'openpolicies',\n",
    " 'openscience',\n",
    " 'opensoftware',\n",
    " 'opentools'\n",
    "}\n",
    "\n",
    "for rater in ['c1', 'c2']:\n",
    "    actions = []\n",
    "    for a in data[f'{rater}_action']:\n",
    "        if a == a:\n",
    "            actions += a\n",
    "    actions = set(actions)\n",
    "\n",
    "    assert len(actions.difference(expected_actions)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21a7036f-b01d-48bb-9729-010d18472225",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_methods = {\n",
    "    'biblio',\n",
    "    'documentreview',\n",
    "    'interview',\n",
    "    'othermeth',\n",
    "    'survey'\n",
    "}\n",
    "\n",
    "for rater in ['c1', 'c2']:\n",
    "    methods = []\n",
    "    for m in data[f'{rater}_method']:\n",
    "        if m == m:\n",
    "            methods += m\n",
    "    methods = set(methods)\n",
    "\n",
    "    assert len(methods.difference(expected_methods)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a881d69-0b72-4488-b850-ba643a63aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_disciplines = {\n",
    "    'natscie',\n",
    "    'engtech',\n",
    "    'med',\n",
    "    'agric',\n",
    "    'socscie',\n",
    "    'hum',\n",
    "    'nonspecificdisc'\n",
    "}\n",
    "\n",
    "for rater in ['c1', 'c2']:\n",
    "    disciplines = []\n",
    "    for d in data[f'{rater}_discipline']:\n",
    "        if d == d:\n",
    "            disciplines += d\n",
    "    disciplines = set(disciplines)\n",
    "\n",
    "    assert len(disciplines.difference(expected_disciplines)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f42dd40-64c6-4db8-9c4a-0fd5e5d4994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_groups = {\n",
    " 'researcher',\n",
    " 'librarian',\n",
    " 'university',\n",
    " 'unisupportstaff',\n",
    " 'publisher',\n",
    " 'policy',\n",
    " 'funder',\n",
    " 'business',\n",
    " 'practitioner',\n",
    " 'othergroup',\n",
    " 'nonspecific'\n",
    "}\n",
    "\n",
    "for rater in ['c1', 'c2']:\n",
    "    groups = []\n",
    "    for g in data[f'{rater}_group']:\n",
    "        if g == g:\n",
    "            groups += g\n",
    "    groups = set(groups)\n",
    "\n",
    "    assert len(groups.difference(expected_groups)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fcda3057-2472-4977-a531-929379c22ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_geo = set(list(pd.read_csv(join(src, \"country_codes.csv\"), \n",
    "                        usecols=[\"iso_a3\"],\n",
    "                        na_values=[\"none\"])[\"iso_a3\"].fillna(\"NA\").values)) \\\n",
    "    | {\"nonspecificgeo\"}\n",
    "\n",
    "for rater in ['c1', 'c2']:\n",
    "    geo = []\n",
    "    for g in data['c1_geo']:\n",
    "        if g == g:\n",
    "            geo += g\n",
    "    geo = set(geo)\n",
    "    \n",
    "    assert len(geo.difference(expected_geo)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5bcc1785-0121-4a9d-9b71-f04e5708603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_entries(entry_list, category):\n",
    "    if entry_list != entry_list:\n",
    "        return np.nan\n",
    "    entry_list = [f\"{category}={l}\" for l in entry_list]\n",
    "    return \"; \".join(entry_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14ee2194-4f50-457c-a579-2a5b0c193634",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"c1_action\"] = data[\"c1_action\"].apply(join_entries, args=[\"a\"])\n",
    "data[\"c2_action\"] = data[\"c2_action\"].apply(join_entries, args=[\"a\"])\n",
    "data[\"c1_method\"] = data[\"c1_method\"].apply(join_entries, args=[\"m\"])\n",
    "data[\"c2_method\"] = data[\"c2_method\"].apply(join_entries, args=[\"m\"])\n",
    "data[\"c1_discipline\"] = data[\"c1_discipline\"].apply(join_entries, args=[\"d\"])\n",
    "data[\"c2_discipline\"] = data[\"c2_discipline\"].apply(join_entries, args=[\"d\"])\n",
    "data[\"c1_group\"] = data[\"c1_group\"].apply(join_entries, args=[\"t\"])\n",
    "data[\"c2_group\"] = data[\"c2_group\"].apply(join_entries, args=[\"t\"])\n",
    "data[\"c1_geo\"] = data[\"c1_geo\"].apply(join_entries, args=[\"geo\"])\n",
    "data[\"c2_geo\"] = data[\"c2_geo\"].apply(join_entries, args=[\"geo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821b987b-8086-4917-8afd-0ecfa5848726",
   "metadata": {},
   "source": [
    "### Export the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f71f72b-d4c4-4721-a66b-0e4924096d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(join(dst, fname + \"_clean.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
