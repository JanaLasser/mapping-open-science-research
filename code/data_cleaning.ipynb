{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "royal-separate",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "limited-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-violence",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fewer-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../data\"\n",
    "dst = \"../data\"\n",
    "fname = \"coded_V1.csv\"\n",
    "data = pd.read_csv(join(src, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-lancaster",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "infrared-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns & drop empty columns\n",
    "data = data\\\n",
    "    .rename(columns={'V=Action':'Action',\n",
    "                     'V=Method':'Method',\n",
    "                     'V=Discipline':'Discipline',\n",
    "                     'V=Group':'Group',\n",
    "                     'V=Geo-Scope':'Geo'})\\\n",
    "    .drop(columns=['Unnamed: 25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "accessory-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of wrong codings and their respective corrections\n",
    "miscodings = {\n",
    "        'policies':'openpolicies', # actions\n",
    "        'openacces':'openaccess', # actions\n",
    "        'interviews':'interview', # methods\n",
    "        'other(casestudies)':'other', # methods\n",
    "        'desk-review':'other', # methods\n",
    "        'deskresearch':'other', # methods\n",
    "        'bibliometric':'biblio', # methods\n",
    "        'documentanalysis':'other', # methods\n",
    "        'socsci':'socscie', # disciplines\n",
    "        'socscie(LIS)':'socsie', # disciplines\n",
    "        'socscie(psychology)':'socscie', # disciplines\n",
    "        'socscie?(informationsciences)':'socscie', # disciplines\n",
    "        'soscie':'socscie', # disciplines\n",
    "        'socsie':'socscie', # disciplines\n",
    "        'librarians':'librarian', # group\n",
    "        'publishers':'publisher', # group\n",
    "        'reseaercher':'researcher', # group\n",
    "        'researchers':'researcher', # group\n",
    "        'all':'nonspecific', # group\n",
    "        'none':'nonspecific', # group\n",
    "        'TW':'TWN', # geo\n",
    "        'IR':'IRN', # geo\n",
    "        'IN':'IND', # geo\n",
    "        'Italy':'ITA', # geo\n",
    "        'PK':'PAK', # geo\n",
    "        'SI':'SVN', # geo\n",
    "        'LA':'SA', # geo\n",
    "        'missing':np.nan # general missing entry code\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "integrated-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(list_string):\n",
    "    '''\n",
    "    Takes a string containing a list of encodings separated by semicolons,\n",
    "    cleans the list, splits it into different entries and returns a list\n",
    "    of entries. Also corrects misspellings along the way.\n",
    "    '''\n",
    "    if list_string != list_string: # NaN check\n",
    "        return np.nan\n",
    "    \n",
    "    list_string = list_string.strip(';') # remove trailing \";\"\n",
    "    raw_entries = list_string.split(';') # split list along \";\"\n",
    "    \n",
    "    entries = []\n",
    "    for e in raw_entries:\n",
    "        e = e.replace(' ', '') # remove white spaces\n",
    "        if '=' in e: # remove leading column code letter\n",
    "            e = e.split('=')[-1]\n",
    "        if e in miscodings.keys(): # clean up wrong encodings\n",
    "            e = miscodings[e]\n",
    "        if e == \"socsie\":\n",
    "            e = \"socscie\"\n",
    "        if e != '' and e == e:\n",
    "            entries.append(e)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "atomic-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the five coded columns\n",
    "cols = ['Action', 'Method', 'Discipline', 'Group', 'Geo']\n",
    "for col in cols:\n",
    "    data[col] = data[col].apply(split_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-airfare",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-retailer",
   "metadata": {},
   "source": [
    "Look at the remaining categories in each coded column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "prescription-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_actions = {\n",
    " 'openaccess',\n",
    " 'opendata',\n",
    " 'openeducation',\n",
    " 'openevaluation',\n",
    " 'openmethod',\n",
    " 'openparticipation',\n",
    " 'openpolicies',\n",
    " 'openscience',\n",
    " 'opensoftware',\n",
    " 'opentools'\n",
    "}\n",
    "actions = []\n",
    "for a in data['Action']:\n",
    "    if a == a:\n",
    "        actions += a\n",
    "actions = set(actions)\n",
    "\n",
    "assert actions == expected_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "soviet-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_methods = {\n",
    "    'biblio',\n",
    "    'documentreview',\n",
    "    'interview',\n",
    "    'other',\n",
    "    'survey'\n",
    "}\n",
    "\n",
    "methods = []\n",
    "for m in data['Method']:\n",
    "    if m == m:\n",
    "        methods += m\n",
    "methods = set(methods)\n",
    "\n",
    "assert methods == expected_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "comparative-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_disciplines = {\n",
    "    'natscie',\n",
    "    'engtech',\n",
    "    'med',\n",
    "    'agric',\n",
    "    'socscie',\n",
    "    'hum',\n",
    "    'nonspecific'\n",
    "}\n",
    "\n",
    "disciplines = []\n",
    "for d in data['Discipline']:\n",
    "    if d == d:\n",
    "        disciplines += d\n",
    "disciplines = set(disciplines)\n",
    "\n",
    "assert disciplines == expected_disciplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "patient-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_groups = {\n",
    " 'researcher',\n",
    " 'librarian',\n",
    " 'university',\n",
    " 'unisupportstaff',\n",
    " 'publisher',\n",
    " 'policy',\n",
    " 'funder',\n",
    " 'business',\n",
    " 'practitioner',\n",
    " 'other',\n",
    " 'nonspecific'\n",
    "}\n",
    "\n",
    "groups = []\n",
    "for g in data['Group']:\n",
    "    if g == g:\n",
    "        groups += g\n",
    "groups = set(groups)\n",
    "\n",
    "assert groups == expected_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "conventional-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = []\n",
    "for g in data['Geo']:\n",
    "    if g == g:\n",
    "        geo += g\n",
    "geo = set(geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "representative-corpus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AF',\n",
       " 'ARE',\n",
       " 'ARG',\n",
       " 'ARM',\n",
       " 'AS',\n",
       " 'AUS',\n",
       " 'AUT',\n",
       " 'BEL',\n",
       " 'BGD',\n",
       " 'BGR',\n",
       " 'BOL',\n",
       " 'BRA',\n",
       " 'BWA',\n",
       " 'CAN',\n",
       " 'CHE',\n",
       " 'CHI',\n",
       " 'CHL',\n",
       " 'CHN',\n",
       " 'CND',\n",
       " 'COL',\n",
       " 'CRI',\n",
       " 'CUB',\n",
       " 'CZE',\n",
       " 'DEN',\n",
       " 'DEU',\n",
       " 'DNK',\n",
       " 'DZA',\n",
       " 'EGY',\n",
       " 'ESP',\n",
       " 'EST',\n",
       " 'ETH',\n",
       " 'EU',\n",
       " 'FIN',\n",
       " 'FRA',\n",
       " 'GBR',\n",
       " 'GHA',\n",
       " 'GNR',\n",
       " 'GRC',\n",
       " 'HKG',\n",
       " 'HNK',\n",
       " 'HRV',\n",
       " 'HUN',\n",
       " 'IDN',\n",
       " 'IND',\n",
       " 'IRL',\n",
       " 'IRN',\n",
       " 'IRQ',\n",
       " 'ISR',\n",
       " 'ITA',\n",
       " 'JAM',\n",
       " 'JPN',\n",
       " 'KEN',\n",
       " 'KOR',\n",
       " 'LBY',\n",
       " 'LKA',\n",
       " 'LTU',\n",
       " 'MAR',\n",
       " 'MEX',\n",
       " 'MYS',\n",
       " 'NA',\n",
       " 'NDL',\n",
       " 'NGA',\n",
       " 'NLD',\n",
       " 'NOR',\n",
       " 'NZL',\n",
       " 'PAK',\n",
       " 'PER',\n",
       " 'PHL',\n",
       " 'PNG',\n",
       " 'POL',\n",
       " 'PRK',\n",
       " 'PRT',\n",
       " 'ROU',\n",
       " 'RUS',\n",
       " 'SA',\n",
       " 'SAU',\n",
       " 'SDN',\n",
       " 'SEN',\n",
       " 'SGP',\n",
       " 'SRB',\n",
       " 'SUI',\n",
       " 'SVK',\n",
       " 'SVN',\n",
       " 'SWE',\n",
       " 'THA',\n",
       " 'TUN',\n",
       " 'TUR',\n",
       " 'TWN',\n",
       " 'TZA',\n",
       " 'UGA',\n",
       " 'URY',\n",
       " 'USA',\n",
       " 'VEN',\n",
       " 'VNM',\n",
       " 'ZAF',\n",
       " 'ZAR',\n",
       " 'ZWE',\n",
       " 'nonspecific'}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-japan",
   "metadata": {},
   "source": [
    "## Dummy code the coded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "extraordinary-south",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_317188/2318456402.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['{}_{}'.format(colname, entry)] = data[colname].apply(lambda x: entry in x if x == x else False)\n"
     ]
    }
   ],
   "source": [
    "for entries, colname in zip([actions, methods, disciplines, groups, geo], cols):\n",
    "    for entry in entries:\n",
    "        if entry == entry:\n",
    "            data['{}_{}'.format(colname, entry)] = data[colname].apply(lambda x: entry in x if x == x else False)\n",
    "            data['{}_{}'.format(colname, entry)] = data['{}_{}'.format(colname, entry)].replace({True:1, False:0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-vermont",
   "metadata": {},
   "source": [
    "## Export the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "supposed-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(join(dst, 'V5_9_1 round coded_220216_clean.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
